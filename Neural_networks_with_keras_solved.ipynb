{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, generators\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import keras.datasets.mnist as mnist\n",
    "import keras.datasets.cifar10 as cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule\n",
    "\n",
    "- What is MNIST? What is CIFAR? (10 minutes)\n",
    "\n",
    "- Loading images and converting them to numpy (5 minutes)\n",
    "\n",
    "- What is Keras? Creating a basic model, Callbacks, saving/loading a model, changing optimizer, loss  (30 minutes)\n",
    "\n",
    "- Mini-Project: Train a feed forward NN on MNIST/CIFAR10 with Keras (No scaling) (15 minutes)\n",
    "\n",
    "- Preprocessing images: Scaling (5 minutes)\n",
    "\n",
    "- Mini-Project: Train a feed forward NN on MNIST/CIFAR10 with Keras (10 minutes)\n",
    "\n",
    "- Preprocessing images: Data augmentation and generators with Keras (10 minutes)\n",
    "\n",
    "- Mini-Project: Train a feed forward NN on MNIST/CIFAR10 using data augmentation with Keras (15 minutes)\n",
    "\n",
    "- Break (20 minutes)\n",
    "\n",
    "- Convolutional NN, Pooling, Flattening, etc  (20 minutes)\n",
    "\n",
    "- How to build a CNN in Keras (10 minutes)\n",
    "\n",
    "- Mini-Project: Train CNN on MNIST/CIFAR10 (30 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is CIFAR10?\n",
    "\n",
    "More information at\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# This is needed to know what each class means\n",
    "label_id_to_class_name = {0:'airplane', 1:'automovile', 2:'bird', 3:'cat', 4:'deer', 5:'dog', \n",
    "                          6:'frog', 7:'horse', 8:'ship', 9:'truck'}\n",
    "\n",
    "print(x_train.shape[0], 'Number of train samples')\n",
    "print(x_test.shape[0], 'Number of test samples')\n",
    "print('x_train shape:', x_train.shape)\n",
    "\n",
    "# plot images\n",
    "for image_id in range(0, 5):\n",
    "    plt.imshow(x_train[image_id])\n",
    "    plt.title(\"The true label is %s\" % label_id_to_class_name[int(y_train[image_id])])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This makes sure the image has the correct order in the axis for Tensorflow, it would be different for Theano backend\n",
    "x_train = x_train.reshape(x_train.shape[0], 32, 32, 3)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32, 32, 3)\n",
    "\n",
    "# Convert values to floats, originally they are integers\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Convert values of labels from 0 to 9 to categorical (one_hot encoding)\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is MNIST?\n",
    "\n",
    "More information at http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(x_train.shape[0], 'Number of train samples')\n",
    "print(x_test.shape[0], 'Number of test samples')\n",
    "print('x_train shape:', x_train.shape)\n",
    "\n",
    "# plot images as gray scale\n",
    "for image_id in range(0, 5):\n",
    "    plt.imshow(x_train[image_id], cmap=plt.get_cmap('gray'))\n",
    "    plt.title(\"The true label is %s\" % str(y_train[image_id]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and reshaping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This makes sure the image has the correct order in the axis for Tensorflow, it would be different for Theano backend\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Convert values to floats, originally they are integers\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Convert values of labels from 0 to 9 to categorical (one_hot encoding)\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and reshaping data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use Keras?\n",
    "\n",
    "\"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. \n",
    "It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "Use Keras if you need a deep learning library that:\n",
    "\n",
    "- Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "\n",
    "- Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "\n",
    "- Runs seamlessly on CPU and GPU.\" (Chollet, F.)\n",
    "\n",
    "More information at https://keras.io Descriptions about the functions and documentation are also taken from this website \n",
    "\n",
    "In summary, it makes your life way easier if you don't require to go to the level of granularity of Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model we have 3 main steps:\n",
    "\n",
    "   - Define your architecture (number of layers, type of layers, activations, etc)\n",
    "    \n",
    "   - Compile your model (Define optimizer, callbacks, etc)\n",
    "    \n",
    "   - Train your model (Fit model to your data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28, 1)))  # Images are a 3D matrix, we have to flatten them to be 1D\n",
    "model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.5)) # drop a unit with  50% probability.\n",
    "model.add(Dense(150, kernel_initializer='orthogonal'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax')) # last layer, this has a softmax to do the classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential model is one layer after the other, no fancy connections. You can also use the Functional API if you want to have connections which skip layers and so on.\n",
    "\n",
    "To add a layer all we have to do is call the add method and then add a layer object.\n",
    "\n",
    "The first layer needs to have the input shape specified. After this the sizes are inferred automatically except for the output layer.\n",
    "\n",
    "We need to use the flatten layer to flatten the input since it is an image, which in this case is a 3 dimensional matrix. It has height, width and depth. In the case of a grayscale image like the ones from MNIST it has 1 dimensional depth. If it is a color image it has 3 layers of depth (Red, Green, Blue).\n",
    "\n",
    "A dense layer is a layer in which all units are connected to all units in the next layer. This is the most usual type of layer. You can specify things like the number of units and how the weights in the units are initialized (kernel_initializer). You can also specify an activation function, by default a linear function is used (No activation).\n",
    "\n",
    "If you want to use dropout in your model you can just add it as an extra layer in between layers or activations. Same thing for batch normalization.\n",
    "\n",
    "At the end we create a layer which will help us with the classification. For this we use a softmax layer in which the number of units will match the number of classes we have in our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Layer\n",
    "\n",
    "keras.layers.Dense(units, activation=None, use_bias=True, \n",
    "                   kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
    "                   kernel_regularizer=None, bias_regularizer=None, \n",
    "                   activity_regularizer=None, \n",
    "                   kernel_constraint=None, bias_constraint=None)\n",
    "                   \n",
    "Arguments\n",
    "\n",
    "- units: Positive integer, dimensionality of the output space.\n",
    "- activation: Activation function to use (see activations). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "- use_bias: Boolean, whether the layer uses a bias vector.\n",
    "- kernel_initializer: Initializer for the kernel weights matrix.\n",
    "- bias_initializer: Initializer for the bias vector.\n",
    "- kernel_regularizer: Regularizer function applied to the kernel weights matrix. (L1, L2)\n",
    "- bias_regularizer: Regularizer function applied to the bias vector.\n",
    "- activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\").\n",
    "- kernel_constraint: Constraint function applied to the kernel weights matrix. (non-negative, etc)\n",
    "- bias_constraint: Constraint function applied to the bias vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model\n",
    "\n",
    "Here we specify the loss, in our case categorical crossentropy. We can add an extra metric we want to measure, like accuracy\n",
    "\n",
    "We also specify the optimizer. Some examples are Adam and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile function\n",
    "\n",
    "Arguments\n",
    "\n",
    "- optimizer: String (name of optimizer) or optimizer object. (SGD, RMSprop, Adam, Adagrad, etc)\n",
    "- loss: String (name of objective function) or objective function. You can also have multiple loss functions\n",
    "- metrics: List of metrics to be evaluated by the model during training and testing. \n",
    "- sample_weight_mode: If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\". None defaults to sample-wise weights (1D).\n",
    "- weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "\n",
    "A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training.\n",
    "\n",
    "Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "my_callbacks = [early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.2, epochs=10, batch_size=100, verbose=1, callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit function\n",
    "\n",
    "Arguments\n",
    "\n",
    "- x: Numpy array of training data. \n",
    "- y: Numpy array of target (label) data.\n",
    "- batch_size: Integer or None. Number of samples per gradient update. If unspecified, it will default to 32.\n",
    "- epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided.\n",
    "- verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "- callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training.\n",
    "- validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data.\n",
    "- validation_data: tuple (x_val, y_val) or tuple (x_val, y_val, val_sample_weights) on which to evaluate the loss and any model metrics at the end of each epoch.\n",
    "- shuffle: Boolean (whether to shuffle the training data before each epoch)\n",
    "- class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only).\n",
    "- sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only).\n",
    "- initial_epoch: Epoch at which to start training (useful for resuming a previous training run).\n",
    "- steps_per_epoch: Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined.\n",
    "- validation_steps: Only relevant if steps_per_epoch is specified. Total number of steps (batches of samples) to validate before stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History object\n",
    "\n",
    "Fit function returns a history object. This object stores information about your model while it was training. For example if you would like to plot the training loss and validation loss across the number of epochs you could do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Keys for history object\", history.history.keys())\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "valid_loss = history.history['val_loss']\n",
    "epochs = list(range(1, len(train_loss)+1))\n",
    "\n",
    "plt.plot(epochs, train_loss, label=\"train loss\")\n",
    "plt.plot(epochs, valid_loss, label=\"validation loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = history.history['acc']\n",
    "valid_accuracy = history.history['val_acc']\n",
    "\n",
    "epochs = list(range(1, len(train_accuracy)+1))\n",
    "\n",
    "plt.plot(epochs, train_accuracy, label=\"train accuracy\")\n",
    "plt.plot(epochs, valid_accuracy, label=\"validation accuracy\")\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "\n",
    "print(predictions.shape)\n",
    "print(np.argmax(predictions, axis=1)[0:10])\n",
    "print(np.argmax(y_test, axis=1)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments\n",
    "\n",
    "- x: the input data, as a Numpy array.\n",
    "- batch_size: Integer. If unspecified, it will default to 32.\n",
    "- verbose: verbosity mode, 0 or 1.\n",
    "- steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None.\n",
    "\n",
    "Returns\n",
    "\n",
    "A Numpy array of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and saving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_path = \"my_new_model.h5\"\n",
    "model.save(model_path)\n",
    "del model  # deletes the existing model\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project: Train you own fully connected neural network to classify handwritten digits\n",
    "\n",
    "- Train a logistic regression to create a benchmark\n",
    "- Train a neural network and compare. Start with a few layers, then experiment with more layers and different parameters.\n",
    "- Are you overfitting? Underfitting? How can you improve your model? Try other hyperparameters or adding regularization. Make some plots to understand the behaviour of your model.\n",
    "- Take a look at samples for which your model is predicting an incorrect label, what do you think is happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression (Benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28, 1)))  # Images are a 3D matrix, we have to flatten them to be 1D\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "log_history = model.fit(x_train, y_train, validation_split=0.2, epochs=10, batch_size=100, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28, 1)))  # Images are a 3D matrix, we have to flatten them to be 1D\n",
    "model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "nn_history = model.fit(x_train, y_train, validation_split=0.2, epochs=10, batch_size=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train_loss = log_history.history['loss']\n",
    "log_valid_loss = log_history.history['val_loss']\n",
    "nn_train_loss = nn_history.history['loss']\n",
    "nn_valid_loss = nn_history.history['val_loss']\n",
    "epochs = list(range(1, len(nn_train_loss)+1))\n",
    "\n",
    "plt.plot(epochs, log_train_loss, label=\"log reg train loss\")\n",
    "plt.plot(epochs, log_valid_loss, label=\"log reg validation loss\")\n",
    "plt.plot(epochs, nn_train_loss, label=\"NN train loss\")\n",
    "plt.plot(epochs, nn_valid_loss, label=\"NN validation loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "log_train_accuracy = log_history.history['acc']\n",
    "log_valid_accuracy = log_history.history['val_acc']\n",
    "nn_train_accuracy = nn_history.history['acc']\n",
    "nn_valid_accuracy = nn_history.history['val_acc']\n",
    "epochs = list(range(1, len(log_train_accuracy)+1))\n",
    "\n",
    "plt.plot(epochs, log_train_accuracy, label=\"log reg train accuracy\")\n",
    "plt.plot(epochs, log_valid_accuracy, label=\"log reg validation accuracy\")\n",
    "plt.plot(epochs, nn_train_accuracy, label=\"NN train accuracy\")\n",
    "plt.plot(epochs, nn_valid_accuracy, label=\"NN validation accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "\n",
    "predicted_labels = np.argmax(predictions, axis=1)  # transform back from one_hot encoding\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "misclassified_samples_positions = np.where(predicted_labels != true_labels)[0]\n",
    "\n",
    "for image_id in misclassified_samples_positions[0:5]:\n",
    "    print(image_id)\n",
    "    print(x_test[image_id].shape)\n",
    "    plt.imshow(x_test[image_id].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "    plt.title(\"The true label is %s but was classified as %s\" % (str(true_labels[image_id]), \n",
    "                                                                 str(predicted_labels[image_id])\n",
    "                                                                ))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale the inputs\n",
    "x_train /= 255.0  # The image is in grayscale and has values between 0 and 255\n",
    "x_test /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project: Train you own fully connected neural network to classify handwritten digits (Now with scaled data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train a logistic regression to create a benchmark\n",
    "- Train a neural network and compare. Start with a few layers, then experiment with more layers and different parameters.\n",
    "- Are you overfitting? Underfitting? How can you improve your model? Try other hyperparameters or adding regularization. Make some plots to understand the behaviour of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra credit\n",
    "\n",
    "- Try shuffling the order of the features before training (move the 5th pixel to a new random position) but do the same shuffling for all samples and retrain you model. How does it perform compared to the original model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28, 1)))  # Images are a 3D matrix, we have to flatten them to be 1D\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "log_history = model.fit(x_train, y_train, validation_split=0.2, epochs=10, batch_size=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28, 1)))  # Images are a 3D matrix, we have to flatten them to be 1D\n",
    "model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "nn_history = model.fit(x_train, y_train, validation_split=0.2, epochs=10, batch_size=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train_loss = log_history.history['loss']\n",
    "log_valid_loss = log_history.history['val_loss']\n",
    "nn_train_loss = nn_history.history['loss']\n",
    "nn_valid_loss = nn_history.history['val_loss']\n",
    "epochs = list(range(1, len(log_train_loss)+1))\n",
    "\n",
    "plt.plot(epochs, log_train_loss, label=\"log reg train loss\")\n",
    "plt.plot(epochs, log_valid_loss, label=\"log reg validation loss\")\n",
    "plt.plot(epochs, nn_train_loss, label=\"NN train loss\")\n",
    "plt.plot(epochs, nn_valid_loss, label=\"NN validation loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "log_train_accuracy = log_history.history['acc']\n",
    "log_valid_accuracy = log_history.history['val_acc']\n",
    "nn_train_accuracy = nn_history.history['acc']\n",
    "nn_valid_accuracy = nn_history.history['val_acc']\n",
    "epochs = list(range(1, len(log_train_accuracy)+1))\n",
    "\n",
    "plt.plot(epochs, log_train_accuracy, label=\"log reg train accuracy\")\n",
    "plt.plot(epochs, log_valid_accuracy, label=\"log reg validation accuracy\")\n",
    "plt.plot(epochs, nn_train_accuracy, label=\"NN train accuracy\")\n",
    "plt.plot(epochs, nn_valid_accuracy, label=\"NN validation accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "\n",
    "predicted_labels = np.argmax(predictions, axis=1)  # transform back from one_hot encoding\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "misclassified_samples_positions = np.where(predicted_labels != true_labels)[0]\n",
    "\n",
    "for image_id in misclassified_samples_positions[0:5]:\n",
    "    print(image_id)\n",
    "    print(x_test[image_id].shape)\n",
    "    plt.imshow(x_test[image_id].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "    plt.title(\"The true label is %s but was classified as %s\" % (str(true_labels[image_id]), \n",
    "                                                                 str(predicted_labels[image_id])\n",
    "                                                                ))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload data before procedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This makes sure the image has the correct order in the axis for Tensorflow, it would be different for Theano backend\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Convert values to floats, originally they are integers\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Convert values of labels from 0 to 9 to categorical (one_hot encoding)\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation and generators\n",
    "\n",
    "Augmenting the data allows us to use more data for training by taking artificial modifications from the samples we already have. One example of this is rotating the image. This will allow our model to learn some rotational invariance to the data and prevent overfitting.\n",
    "\n",
    "To train a model when a dataset is too large to load in RAM memory we can use generators. Generators allow us to only pass a small portion of the data at a time so it doesn't use all the memory from the computer. In addition once it loops through all the data we can set the generator so it starts again from the beginning. This allows us to loop infinitely through the data.\n",
    "\n",
    "Keras gives us a nice tool for augmenting data which uses a generator. It uses a generator because the modifications are done on the fly and it would take too much memory to save the augmented dataset. Keras also provides us with a method to fit the model when we use a generator, fit_generator(). This method requires us to specify how many batches are equivalent to an epoch so the model knows when to do updates, otherwise since the generator loops through the data, it would go on training infinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255,\n",
    "                                    #featurewise_center=True,\n",
    "                                    #featurewise_std_normalization=True,\n",
    "                                    rotation_range=10,\n",
    "                                    width_shift_range=0.1,\n",
    "                                    height_shift_range=0.1,\n",
    "                                    #horizontal_flip=True,\n",
    "                                    zoom_range=0.1\n",
    "                                    )\n",
    "\n",
    "# we git the model to the data. This needed for calculating mean and std.\n",
    "data_generator.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_generator = data_generator.flow(x_train[0:40000], y_train[0:40000], batch_size=100)\n",
    "augmented_batch, labels = augmented_data_generator.next()\n",
    "\n",
    "print(augmented_batch.shape)\n",
    "print(augmented_batch[0].shape)\n",
    "print(labels[0])\n",
    "\n",
    "# plot images as gray scale\n",
    "for image_id in range(0, 5):\n",
    "    plt.imshow(augmented_batch[image_id].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "    plt.title(\"The true label is %s\" % str(np.argmax(labels[image_id])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model using a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(data_generator.flow(x_train[0:40000], y_train[0:40000], batch_size=100), \n",
    "                              steps_per_epoch=40000/100,\n",
    "                              validation_data=(x_train[40000:], y_train[40000:]), \n",
    "                              epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments\n",
    "\n",
    "- generator: A generator. The output of the generator must be either\n",
    "a tuple (inputs, targets)\n",
    "a tuple (inputs, targets, sample_weights). All arrays should contain the same number of samples. The generator is expected to loop over its data indefinitely. An epoch finishes when steps_per_epoch batches have been seen by the model.\n",
    "- steps_per_epoch: Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of samples of your dataset divided by the batch size.\n",
    "- epochs: Integer, total number of iterations on the data. \n",
    "- verbose: Verbosity mode, 0, 1, or 2.\n",
    "- callbacks: List of callbacks to be called during training.\n",
    "- validation_data: This can be either\n",
    "A generator for the validation data\n",
    "A tuple (inputs, targets)\n",
    "A tuple (inputs, targets, sample_weights).\n",
    "- validation_steps: Only relevant if validation_data is a generator. Number of steps to yield from validation generator at the end of every epoch. It should typically be equal to the number of samples of your validation dataset divided by the batch size. Optional for Sequence: if unspecified, will use the len(validation_data) as a number of steps.\n",
    "- class_weight: Dictionary mapping class indices to a weight for the class.\n",
    "- max_queue_size: Maximum size for the generator queue\n",
    "- workers: Maximum number of processes to spin up\n",
    "- use_multiprocessing: if True, use process based threading.\n",
    "- initial_epoch: Epoch at which to start training (useful for resuming a previous training run).\n",
    "\n",
    "Returns\n",
    "\n",
    "A History object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project: Train you own fully connected neural network to classify handwritten digits (Now with augmented data)\n",
    "\n",
    "- Train a logistic regression to create a benchmark\n",
    "- Train a neural network and compare. Start with a few layers, then experiment with more layers and different parameters.\n",
    "- Are you overfitting? Underfitting? How can you improve your model? Try other hyperparameters or adding regularization. Make some plots to understand the behaviour of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28, 1)))  # Images are a 3D matrix, we have to flatten them to be 1D\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "log_history = model.fit_generator(data_generator.flow(x_train[0:40000], y_train[0:40000], batch_size=100), \n",
    "                              steps_per_epoch=40000/100,\n",
    "                              validation_data=(x_train[40000:], y_train[40000:]), \n",
    "                              epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28, 1)))  # Images are a 3D matrix, we have to flatten them to be 1D\n",
    "model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(10, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "nn_history = model.fit_generator(data_generator.flow(x_train[0:40000], y_train[0:40000], batch_size=100), \n",
    "                              steps_per_epoch=40000/100,\n",
    "                              validation_data=(x_train[40000:], y_train[40000:]), \n",
    "                              epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train_loss = log_history.history['loss']\n",
    "log_valid_loss = log_history.history['val_loss']\n",
    "nn_train_loss = nn_history.history['loss']\n",
    "nn_valid_loss = nn_history.history['val_loss']\n",
    "epochs = list(range(1, len(log_train_loss)+1))\n",
    "\n",
    "plt.plot(epochs, log_train_loss, label=\"log reg train loss\")\n",
    "plt.plot(epochs, log_valid_loss, label=\"log reg validation loss\")\n",
    "plt.plot(epochs, nn_train_loss, label=\"NN train loss\")\n",
    "plt.plot(epochs, nn_valid_loss, label=\"NN validation loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "log_train_accuracy = log_history.history['acc']\n",
    "log_valid_accuracy = log_history.history['val_acc']\n",
    "nn_train_accuracy = nn_history.history['acc']\n",
    "nn_valid_accuracy = nn_history.history['val_acc']\n",
    "epochs = list(range(1, len(log_train_accuracy)+1))\n",
    "\n",
    "plt.plot(epochs, log_train_accuracy, label=\"log reg train accuracy\")\n",
    "plt.plot(epochs, log_valid_accuracy, label=\"log reg validation accuracy\")\n",
    "plt.plot(epochs, nn_train_accuracy, label=\"NN train accuracy\")\n",
    "plt.plot(epochs, nn_valid_accuracy, label=\"NN validation accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Convolutional neural networks are similar to the previous networks we saw with a main difference, they start with the assumption that the input will be an image and optimize the architecture for that assumption. Now they are used in many other contexts besides images, but this is what they were created for.\n",
    "\n",
    "Convolutional layer define a kernel (weight matrix) which is then multiplied element by element with a section of the input of the same size. Sum all the resulting values. Move the kernel a number of pixels equal to a defined stride size and repeat until you go throught the whole image. Afterwards apply an activation function to each one of the values. Now repeat all of this for each filter in your layer.\n",
    "\n",
    "<img src=\"images/conv_layer.gif\" alt=\"convolutional layer\">\n",
    "Animation from Karpathy (http://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "A key aspect is the fact that the weights from the kernel are the same when applied in all sections of the input for the same filter. This means the convolutional layer has way less free parameters than a fully connected layer. This also gives the model location invariance since the filters will activate in the same way if the object is in the top left of the image or at the bottom right.\n",
    "\n",
    "Images are 3D objects which have height, length and depth. In the case of grayscale images the depth is 1, however in color images you have 3 channels, red, green and blue. So convolutional layers take as input a 3D tensor and outputs a 3D tensor.\n",
    "\n",
    "<img src=\"images/cnn.jpeg\" alt=\"convolutional layer\">\n",
    "Image from Karpathy (http://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "Notice the convolutional operation will reduce the size of the input. Depending on your architecture you might want to keep the size constant, you can do this by includding padding in the borders of the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers\n",
    "\n",
    "The idea of the pooling layer is to reduce the size of the input and also help regularize the model. The most famous type of pooling is max pooling. In this case you select an area which is the size of the filter and from it you will only pass through the maximum values among the inputs. Then move the filter a specific stride size and repeat.\n",
    "\n",
    "<img src=\"images/maxpool.jpeg\" alt=\"convolutional layer\">\n",
    "Image from Karpathy (http://cs231n.github.io/convolutional-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice it is best to use blocks of stacked convolutional layers with rectified linear units followed by a max pooling operation. Then at the end have a small number of fully connected layers and the output layer.\n",
    "\n",
    "Each convolutional layer usually has a small filter size (2, 2), (3, 3) and use max pooling with stride (2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', \n",
    "                    activation=None, use_bias=True, \n",
    "                    kernel_initializer='glorot_uniform', bias_initializer='zeros', \n",
    "                    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
    "                    kernel_constraint=None, bias_constraint=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Conv2D(32, (3, 3), padding='same')  # Keeps output of the same size as input\n",
    "Conv2D(32, (3, 3)) # Reduces the size of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments\n",
    "\n",
    "- filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "- kernel_size: An integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
    "- strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the width and height. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.\n",
    "- padding: one of \"valid\" or \"same\" (case-insensitive).\n",
    "- activation: Activation function to use (see activations). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "- use_bias: Boolean, whether the layer uses a bias vector.\n",
    "- kernel_initializer: Initializer for the kernel weights matrix (see initializers).\n",
    "- bias_initializer: Initializer for the bias vector (see initializers).\n",
    "- kernel_regularizer: Regularizer function applied to the kernel weights matrix (see regularizer).\n",
    "- bias_regularizer: Regularizer function applied to the bias vector (see regularizer).\n",
    "- activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer).\n",
    "- kernel_constraint: Constraint function applied to the kernel matrix (see constraints).\n",
    "- bias_constraint: Constraint function applied to the bias vector (see constraints).\n",
    "\n",
    "Input shape\n",
    "\n",
    "4D tensor with shape: (samples, channels, rows, cols) if data_format='channels_first' or 4D tensor with shape: (samples, rows, cols, channels) if data_format='channels_last'.\n",
    "\n",
    "Output shape\n",
    "\n",
    "4D tensor with shape: (samples, filters, new_rows, new_cols) if data_format='channels_first' or 4D tensor with shape: (samples, new_rows, new_cols, filters) if data_format='channels_last'. rows and cols values might have changed due to padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')  # This will reduce the size of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments\n",
    "\n",
    "- pool_size: integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension. If only one integer is specified, the same window length will be used for both dimensions.\n",
    "- strides: Integer, tuple of 2 integers, or None. Strides values. If None, it will default to pool_size.\n",
    "- padding: One of \"valid\" or \"same\" (case-insensitive).\n",
    "\n",
    "Input shape\n",
    "\n",
    "4D tensor with shape: (batch_size, rows, cols, channels)\n",
    "\n",
    "Output shape\n",
    "\n",
    "4D tensor with shape: (batch_size, pooled_rows, pooled_cols, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project: Train you own convolutional neural network to classify handwritten digits (With or without augmented data)\n",
    "\n",
    "- Train a logistic regression to create a benchmark\n",
    "- Train a convolutional neural network and compare it with the logistic regression and fully connected neural network. Start with a few layers, then experiment with more layers and different parameters.\n",
    "- Are you overfitting? Underfitting? How can you improve your model? Try other hyperparameters or adding regularization. Make some plots to understand the behaviour of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 158s - loss: 0.4987 - acc: 0.8372 - val_loss: 0.3219 - val_acc: 0.9799\n",
      "Epoch 2/10\n",
      " - 156s - loss: 0.1578 - acc: 0.9515 - val_loss: 0.2249 - val_acc: 0.9859\n",
      "Epoch 3/10\n",
      " - 156s - loss: 0.1122 - acc: 0.9653 - val_loss: 0.1746 - val_acc: 0.9890\n",
      "Epoch 4/10\n",
      " - 156s - loss: 0.0967 - acc: 0.9705 - val_loss: 0.1823 - val_acc: 0.9886\n",
      "Epoch 5/10\n",
      " - 170s - loss: 0.0834 - acc: 0.9739 - val_loss: 0.1648 - val_acc: 0.9897\n",
      "Epoch 6/10\n",
      " - 161s - loss: 0.0759 - acc: 0.9765 - val_loss: 0.1394 - val_acc: 0.9913\n",
      "Epoch 7/10\n",
      " - 155s - loss: 0.0692 - acc: 0.9783 - val_loss: 0.1536 - val_acc: 0.9904\n",
      "Epoch 8/10\n",
      " - 155s - loss: 0.0662 - acc: 0.9793 - val_loss: 0.1256 - val_acc: 0.9920\n",
      "Epoch 9/10\n",
      " - 155s - loss: 0.0613 - acc: 0.9815 - val_loss: 0.1355 - val_acc: 0.9916\n",
      "Epoch 10/10\n",
      " - 155s - loss: 0.0590 - acc: 0.9822 - val_loss: 0.1201 - val_acc: 0.9925\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "cnn_history = model.fit_generator(data_generator.flow(x_train[0:40000], y_train[0:40000], batch_size=100), \n",
    "                              steps_per_epoch=40000/100,\n",
    "                              validation_data=(x_train[40000:], y_train[40000:]), \n",
    "                              epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_train_loss = log_history.history['loss']\n",
    "log_valid_loss = log_history.history['val_loss']\n",
    "nn_train_loss = nn_history.history['loss']\n",
    "nn_valid_loss = nn_history.history['val_loss']\n",
    "cnn_train_loss = cnn_history.history['loss']\n",
    "cnn_valid_loss = cnn_history.history['val_loss']\n",
    "epochs = list(range(1, len(log_train_loss)+1))\n",
    "\n",
    "plt.plot(epochs, log_train_loss, label=\"log reg train loss\")\n",
    "plt.plot(epochs, log_valid_loss, label=\"log reg validation loss\")\n",
    "plt.plot(epochs, nn_train_loss, label=\"NN train loss\")\n",
    "plt.plot(epochs, nn_valid_loss, label=\"NN validation loss\")\n",
    "plt.plot(epochs, cnn_train_loss, label=\"CNN train loss\")\n",
    "plt.plot(epochs, cnn_valid_loss, label=\"CNN validation loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "log_train_accuracy = log_history.history['acc']\n",
    "log_valid_accuracy = log_history.history['val_acc']\n",
    "nn_train_accuracy = nn_history.history['acc']\n",
    "nn_valid_accuracy = nn_history.history['val_acc']\n",
    "cnn_train_accuracy = cnn_history.history['acc']\n",
    "cnn_valid_accuracy = cnn_history.history['val_acc']\n",
    "epochs = list(range(1, len(log_train_accuracy)+1))\n",
    "\n",
    "plt.plot(epochs, log_train_accuracy, label=\"log reg train accuracy\")\n",
    "plt.plot(epochs, log_valid_accuracy, label=\"log reg validation accuracy\")\n",
    "plt.plot(epochs, nn_train_accuracy, label=\"NN train accuracy\")\n",
    "plt.plot(epochs, nn_valid_accuracy, label=\"NN validation accuracy\")\n",
    "plt.plot(epochs, cnn_train_accuracy, label=\"CNN train accuracy\")\n",
    "plt.plot(epochs, cnn_valid_accuracy, label=\"CNN validation accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3Keras2_env]",
   "language": "python",
   "name": "Python [py3Keras2_env]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
